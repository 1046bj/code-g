import requests
import sqlite3
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from urllib.parse import unquote

# ==========================================
# 1. ì„¤ì •
# ==========================================
API_KEY_RAW = "d89c618d3ff720dfaa7da509d296a9c8d32f2ec90592ffa1e3c0a73f32dce7f4"
API_KEY_DECODED = unquote(API_KEY_RAW)
DB_NAME = "kstartup.db"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def init_db():
    conn = sqlite3.connect(DB_NAME)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS notices (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source TEXT, title TEXT, category TEXT, region TEXT,
            start_date TEXT, end_date TEXT, agency TEXT, target TEXT,
            url TEXT UNIQUE, crawled_at DATETIME
        )
    """)
    conn.commit()
    conn.close()

# ==========================================
# 2. [í•µì‹¬] ìŠ¤ë§ˆíŠ¸ ì¬ë¶„ë¥˜ í•¨ìˆ˜
# ==========================================
def smart_classify(notice):
    """
    K-Startupì´ë‚˜ ì¡°ë‹¬ì²­ ë°ì´í„° ì¤‘ì—ì„œ 
    ì‹¤ì œ ì£¼ê´€ê¸°ê´€ì´ 'ì¤‘ê¸°ë¶€'ë‚˜ 'NIPA'ì¸ ê²ƒì„ ì°¾ì•„ë‚´ì–´ ì†ŒìŠ¤ë¥¼ ë³€ê²½í•¨
    """
    title = notice['title']
    agency = notice['agency']
    
    # 1. ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€(ì¤‘ê¸°ë¶€) ì‹ë³„
    if "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€" in agency or "ì§€ë°©ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ì²­" in agency:
        notice['source'] = "ì¤‘ê¸°ë¶€"
    
    # 2. NIPA(ì •ë³´í†µì‹ ì‚°ì—…ì§„í¥ì›) ì‹ë³„
    elif "ì •ë³´í†µì‹ ì‚°ì—…ì§„í¥ì›" in agency or "NIPA" in title or "ì •ë³´í†µì‹ ì‚°ì—…ì§„í¥ì›" in title:
        notice['source'] = "NIPA"
        
    return notice

def normalize_data(source, item):
    notice = {"source": source, "title": "-", "category": "ì§€ì›ì‚¬ì—…", "region": "ì „êµ­", 
              "start_date": "-", "end_date": "-", "agency": "-", "target": "ì œí•œì—†ìŒ", "url": "-"}
    try:
        # [1] ì°½ì—…ì§„í¥ì› (K-Startup)
        if source == "ì°½ì—…ì§„í¥ì›":
            notice["title"] = item.get('biz_pbanc_nm', 'ì œëª©ì—†ìŒ')
            notice["url"] = item.get('detl_pg_url') or item.get('biz_gdnc_url') or '-'
            notice["start_date"] = item.get('pbanc_rcpt_bgng_dt', '-')
            notice["agency"] = item.get('sprv_inst', 'ì°½ì—…ì§„í¥ì›') # ì£¼ê´€ê¸°ê´€
            notice["category"] = item.get('supt_biz_clsfc', 'ì°½ì—…ì§€ì›')

        # [2] ì¡°ë‹¬ì²­ (ë‚˜ë¼ì¥í„°)
        elif source == "ì¡°ë‹¬ì²­":
            notice["title"] = item.get('bidNtceNm', 'ì œëª©ì—†ìŒ')
            notice["url"] = item.get('bidNtceDtlUrl', '-')
            notice["start_date"] = item.get('bidNtceDt', '-')[:10]
            notice["agency"] = item.get('dminsttNm', 'ì¡°ë‹¬ì²­') # ìˆ˜ìš”ê¸°ê´€
            notice["category"] = "ê³µê³µì…ì°°"

        # [3] ê³¼ê¸°ì •í†µë¶€
        elif source == "ê³¼ê¸°ì •í†µë¶€":
            notice["title"] = item.get('subject', 'ì œëª©ì—†ìŒ')
            notice["url"] = item.get('viewUrl', '-')
            notice["start_date"] = item.get('pressDt', '-')
            notice["agency"] = item.get('deptName', 'ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€')

        # [4] ì‹ì•½ì²˜
        elif source == "ì‹ì•½ì²˜":
            notice["title"] = item.get('PBLANC_NM', 'ì œëª©ì—†ìŒ')
            p_no = item.get('PBLANC_NO')
            notice["url"] = f"https://www.mfds.go.kr/search/search.do?searchTerm={p_no}" if p_no else "-"
            notice["agency"] = "ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜"
            notice["start_date"] = item.get('RCEPT_BEGIN_DTE', '-')

        # [5] NIPA (ì§ì ‘ ìˆ˜ì§‘ìš© - ë³´ì¡°)
        elif source == "NIPA":
            notice["title"] = item.get('ì œëª©') or item.get('ê³µê³ ëª…') or 'ì œëª©ì—†ìŒ'
            notice["url"] = item.get('ë§í¬') or 'https://www.nipa.kr'
            notice["agency"] = "ì •ë³´í†µì‹ ì‚°ì—…ì§„í¥ì›"

    except: pass
    
    if notice['title'] == '-' or notice['title'] == 'ì œëª©ì—†ìŒ': return None
    
    # [ì¤‘ìš”] ìŠ¤ë§ˆíŠ¸ ì¬ë¶„ë¥˜ ì‹¤í–‰
    notice = smart_classify(notice)
    
    return notice

# ==========================================
# 3. ê¸°ê´€ë³„ ìˆ˜ì§‘ (ì•ˆì •ì ì¸ 4ëŒ€ì¥ ìœ„ì£¼)
# ==========================================

def get_kstartup():
    print("ğŸ“¡ [1/4] ì°½ì—…ì§„í¥ì› (K-Startup) ìˆ˜ì§‘...")
    # K-Startupì—ëŠ” ì¤‘ê¸°ë¶€, NIPA, ê³¼ê¸°ë¶€ ê³µê³ ê°€ ëª¨ë‘ ëª¨ì—¬ìˆìŠµë‹ˆë‹¤.
    url = "https://apis.data.go.kr/B552735/kisedKstartupService01/getAnnouncementInformation01"
    params = {"serviceKey": API_KEY_RAW, "page": "1", "perPage": "1000", "returnType": "json", "rcrt_prgs_yn": "Y"}
    try:
        res = requests.get(url, params=params, headers=HEADERS, timeout=10)
        items = res.json().get('data', [])
        return [normalize_data("ì°½ì—…ì§„í¥ì›", i) for i in items]
    except: return []

def get_nara():
    print("ğŸ“¡ [2/4] ì¡°ë‹¬ì²­ (ë‚˜ë¼ì¥í„°) ìˆ˜ì§‘...")
    # ë‚˜ë¼ì¥í„°ì—ë„ NIPA ìš©ì—­ ì…ì°°ì´ ì˜¬ë¼ì˜µë‹ˆë‹¤.
    url = "http://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoServcPPSSrch"
    now = datetime.now()
    ago = now - timedelta(days=14)
    params = {"serviceKey": API_KEY_RAW, "numOfRows": "800", "pageNo": "1", "type": "json",
              "inqryDiv": "1", "inqryBgnDt": ago.strftime("%Y%m%d")+"0000", "inqryEndDt": now.strftime("%Y%m%d")+"2359"}
    try:
        res = requests.get(url, params=params, headers=HEADERS, timeout=10)
        items = res.json().get('response', {}).get('body', {}).get('items', [])
        return [normalize_data("ì¡°ë‹¬ì²­", i) for i in items]
    except: return []

def get_msit():
    print("ğŸ“¡ [3/4] ê³¼ê¸°ì •í†µë¶€ ìˆ˜ì§‘...")
    url = "http://apis.data.go.kr/1721000/msitannouncementinfo/businessAnnouncMentList"
    params = {"serviceKey": API_KEY_RAW, "numOfRows": "100", "pageNo": "1"}
    try:
        res = requests.get(url, params=params, headers=HEADERS, timeout=10)
        root = ET.fromstring(res.content)
        items = [{child.tag: child.text for child in item} for item in root.findall('.//item')]
        return [normalize_data("ê³¼ê¸°ì •í†µë¶€", i) for i in items]
    except: return []

def get_mfds():
    print("ğŸ“¡ [4/4] ì‹ì•½ì²˜ ìˆ˜ì§‘...")
    url = "https://apis.data.go.kr/1471057/RNDBSNSPBLANC01/getRndbsnspblanc01"
    params = {"serviceKey": API_KEY_DECODED, "pageNo": "1", "numOfRows": "200", "type": "xml"}
    try:
        res = requests.get(url, params=params, headers=HEADERS, timeout=30)
        root = ET.fromstring(res.content)
        items = [{child.tag: child.text for child in item} for item in root.findall('.//item')]
        return [normalize_data("ì‹ì•½ì²˜", i) for i in items]
    except: return []

# ==========================================
# 4. ì‹¤í–‰ ë° ì €ì¥
# ==========================================
def save_to_db(data_list):
    conn = sqlite3.connect(DB_NAME)
    cur = conn.cursor()
    new_cnt = 0
    for d in data_list:
        if not d: continue
        try:
            cur.execute("""
                INSERT OR IGNORE INTO notices 
                (source, title, category, region, start_date, end_date, agency, target, url, crawled_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                d['source'], d['title'], d['category'], d['region'],
                d['start_date'], d['end_date'], d['agency'], d['target'],
                d['url'], datetime.now()
            ))
            if cur.rowcount > 0: new_cnt += 1
        except: pass
    conn.commit()
    conn.close()
    return new_cnt

def run_crawler():
    print("\nğŸš€ [Code-G] í†µí•© ìˆ˜ì§‘ & ìŠ¤ë§ˆíŠ¸ ë¶„ë¥˜ ì—”ì§„ ê°€ë™")
    init_db()
    
    all_data = []
    
    # 1. ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘
    k_data = get_kstartup()
    nara_data = get_nara()
    msit_data = get_msit()
    mfds_data = get_mfds()
    
    all_data.extend(k_data)
    all_data.extend(nara_data)
    all_data.extend(msit_data)
    all_data.extend(mfds_data)
    
    if all_data:
        saved = save_to_db(all_data)
        
        # 2. ê²°ê³¼ ë¶„ì„ (ì¬ë¶„ë¥˜ëœ ê²°ê³¼ í™•ì¸)
        sources = {}
        for d in all_data:
            if not d: continue
            src = d['source']
            sources[src] = sources.get(src, 0) + 1
            
        print(f"\nâœ… ìµœì¢… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_data)}ê°œ (ì‹ ê·œ {saved}ê°œ)")
        print("-" * 40)
        print(f"ğŸ“Š [ìŠ¤ë§ˆíŠ¸ ë¶„ë¥˜ ê²°ê³¼]")
        print(f"   - ì¤‘ê¸°ë¶€ (K-Startup ì¶”ì¶œ í¬í•¨): {sources.get('ì¤‘ê¸°ë¶€', 0)}ê°œ")
        print(f"   - NIPA (ì£¼ê´€ê¸°ê´€ ì¶”ì¶œ):       {sources.get('NIPA', 0)}ê°œ")
        print(f"   - ê³¼ê¸°ì •í†µë¶€:                 {sources.get('ê³¼ê¸°ì •í†µë¶€', 0)}ê°œ")
        print(f"   - ì‹ì•½ì²˜:                     {sources.get('ì‹ì•½ì²˜', 0)}ê°œ")
        print(f"   - ì°½ì—…ì§„í¥ì›:                 {sources.get('ì°½ì—…ì§„í¥ì›', 0)}ê°œ")
        print(f"   - ì¡°ë‹¬ì²­:                     {sources.get('ì¡°ë‹¬ì²­', 0)}ê°œ")
        print("-" * 40)
        
    else:
        print("\nâŒ ë°ì´í„° ì—†ìŒ")

if __name__ == "__main__":
    run_crawler()